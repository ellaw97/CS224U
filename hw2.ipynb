{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework covers material from the sentiment classification unit. The primary value of doing the work is that it provides more hands-on experience with the dataset and the models we explored. All the code you write has potential value in the bake-off for this unit as well.\n",
    "\n",
    "Submission URL: https://canvas.stanford.edu/courses/83399/quizzes/50657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellaw/anaconda3/envs/nlu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn import naive_bayes\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import scipy.stats\n",
    "# from sgd_classifier import BasicSGDClassifier\n",
    "# from tf_shallow_neural_classifier import TfShallowNeuralClassifier\n",
    "from tf_rnn_classifier import TfRNNClassifier\n",
    "import sst\n",
    "import os\n",
    "import numpy as np\n",
    "import vsm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from nltk.util import bigrams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tf_shallow_neural_classifier import TfShallowNeuralClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsmdata_home = 'vsmdata'\n",
    "\n",
    "glove_home = os.path.join(vsmdata_home, 'glove.6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 1–4: Reproducing a Socher et al's NaiveBayes baseline [4 points]\n",
    "\n",
    "[Socher et al. 2013](http://www.aclweb.org/anthology/D/D13/D13-1170.pdf) compare against a Naive Bayes baseline with bigram features. See how close you can come to reproducing the performance of this model on the binary, root-only problem (values in the rightmost column of their Table 1, rows 1 and 3).\n",
    "\n",
    "Specific tasks:\n",
    "\n",
    "1. Write a bigrams feature function, on the model of `unigrams_phi`. Call this `bigrams_phi`. In writing this function, ensure that each example is padded with start and end symbols (say, `<S>` and `</S>`), so that these contexts are properly reflected in the feature space.\n",
    "\n",
    "1. Write a function `fit_nb_classifier_with_crossvalidation` that serves as a wrapper for `sklearn.naive_bayes.MultinomialNB` and searches over these values for the smoothing parameter `alpha`: `[0.1, 0.5, 1.0, 2.0]`, using 3-fold cross-validation.\n",
    "\n",
    "1. Use `sst.experiment` to run the experiments, assessing against `dev_reader`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. Your `bigrams_phi`\n",
    "1. Your `fit_nb_classifier`\n",
    "1. Your call to `sst.experiment` \n",
    "1. The average F1 score that `sst.experiment` reported.\n",
    "\n",
    "__A note on performance__: in our experience, the bigrams Naive Bayes model gets around 0.75. It's fine to submit answers with comparable numbers; the Socher et al. baselines are very strong. We're not evaluating how good your model is; we want to see your code, and we're interested to see what the range of F1 scores is across the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_phi(tree):\n",
    "    children = tree.leaves()\n",
    "    children.append('</S>')         \n",
    "    children.insert(0, '<S>')    \n",
    "    bigr = list(bigrams(children))\n",
    "    return Counter(bigr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb20 = pd.read_csv(\n",
    "    os.path.join(vsmdata_home, 'imdb_window20-flat.csv.gz'), index_col=0)\n",
    "imdb20_ppmi = vsm.pmi(imdb20, positive=False) \n",
    "imdb20_ppmi_svd = vsm.lsa(imdb20_ppmi, k=50) \n",
    "imdb_lookup = dict(zip(imdb20_ppmi_svd.index, imdb20_ppmi_svd.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_phi(tree, np_func=np.sum):\n",
    "    return vsm_leaves_phi(tree, imdb_lookup, np_func=np_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb_classifier_with_crossvalidation_tfidf(X, y):\n",
    "    tfidf = vsm.tfidf(X)\n",
    "    basemod = naive_bayes.MultinomialNB()\n",
    "    cv = 20\n",
    "    param_grid = {'alpha': [5, 5.5]}    \n",
    "    return sst.fit_classifier_with_crossvalidation(tfidf, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb_classifier_with_crossvalidation(X, y):\n",
    "    basemod = naive_bayes.MultinomialNB()\n",
    "    cv = 3\n",
    "    param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}    \n",
    "    return sst.fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'alpha': 0.5}\n",
      "Best score: 0.725\n",
      "Accuracy: 0.748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.761     0.708     0.734       428\n",
      "   positive      0.736     0.786     0.760       444\n",
      "\n",
      "avg / total      0.749     0.748     0.747       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    bigrams_phi,                      # Free to write your own!\n",
    "    fit_nb_classifier_with_crossvalidation,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func)  # Fixed.   uni_phi-0.786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb_classifier_with_crossvalidation_ppmi(X, y):\n",
    "    ppmi = vsm.pmi(X)\n",
    "    basemod = naive_bayes.MultinomialNB()\n",
    "    cv = 3\n",
    "    param_grid = {'alpha': [3.4, 3.45, 3.35]}    \n",
    "    return sst.fit_classifier_with_crossvalidation(ppmi, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'alpha': 3.4}\n",
      "Best score: 0.777\n",
      "Accuracy: 0.792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.808     0.757     0.782       428\n",
      "   positive      0.779     0.827     0.802       444\n",
      "\n",
      "avg / total      0.793     0.792     0.792       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,                      # Free to write your own!\n",
    "    fit_nb_classifier_with_crossvalidation_ppmi,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func)  # Fixed.   uni_phi-0.786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5–6: A more powerful vector-summing baseline [4 points]\n",
    "\n",
    "In the section [Distributed representations as features](sst_03_neural_networks.ipynb#Distributed-representations-as-features), we looked at a baseline for the binary SST problem in which each example is modeled as the sum of its 50-dimensional GloVe representations. A `LogisticRegression` model was used for prediction. A neural network might do better here, since there might be complex relationships between the input feature dimensions that a linear classifier can't learn. \n",
    "\n",
    "To address this question, rerun the experiment with `tf_shallow_neural_classifier.TfShallowNeuralClassifier` as the classifier. Specs:\n",
    "* Use `sst.experiment` to conduct the experiment. \n",
    "* Using 3-fold cross-validation, exhaustively explore this set of hyperparameter combinations:\n",
    "  * The hidden dimensionality at 50, 100, and 200.\n",
    "  * The hidden activation function as `tf.nn.tanh` or `tf.nn.relu`.\n",
    "* (For all other parameters to `TfShallowNeuralClassifier`, use the defaults.)\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "* Your average F1 score according to `sst.experiment`. \n",
    "* The optimal hyperparameters chosen in your experiment. (You can just paste in the dict that `sst._experiment` prints.)\n",
    "\n",
    "No need to include your supporting code. \n",
    "\n",
    "We're not evaluating the quality of your model. (We've specified the protocols completely, but there will still be a  lot of variation in the results.) However, the primary goal of this question is to get you thinking more about this strikingly good baseline feature representation scheme for SST, so we're sort of hoping you feel compelled to try out variations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.tree\n",
    "        The tree to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    defaultdict\n",
    "        A map from strings to their counts in `tree`. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(glove_home, 'glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_leaves_phi(tree, lookup, np_func=np.sum):\n",
    "    allvecs = np.array([lookup[w] for w in tree.leaves() if w in lookup])    \n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_leaves_phi(tree, np_func=np.sum):\n",
    "    return vsm_leaves_phi(tree, glove_lookup, np_func=np_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_with_crossvalidation(X, y):\n",
    "    basemod = LogisticRegression()\n",
    "    cv = 10\n",
    "    param_grid = {'fit_intercept': [True, False], \n",
    "                  'C': [2.3, 2.2, 2.1, 2.4],\n",
    "                  'penalty': ['l1','l2'],\n",
    "                 'intercept_scaling': [1, 1.1, 0.9]}    \n",
    "    return sst.fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_shallow_nn_with_crossvalidation(X, y):\n",
    "    basemod = TfShallowNeuralClassifier()\n",
    "    cv = 3\n",
    "    param_grid = {'hidden_dim': [200], 'hidden_activation': [tf.nn.relu]}    \n",
    "    return sst.fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 100: loss: 2.8135803341865546"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'hidden_activation': <function relu at 0x1a15e55950>, 'hidden_dim': 200}\n",
      "Best score: 0.641\n",
      "Accuracy: 0.726\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.701     0.736     0.718       985\n",
      "   positive      0.750     0.717     0.733      1091\n",
      "\n",
      "avg / total      0.727     0.726     0.726      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    glove_leaves_phi,\n",
    "    fit_shallow_nn_with_crossvalidation,\n",
    "    class_func=sst.binary_class_func,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):        \n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'C': 2.3, 'fit_intercept': False, 'intercept_scaling': 1, 'penalty': 'l2'}\n",
      "Best score: 0.781\n",
      "Accuracy: 0.768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.780     0.736     0.757       428\n",
      "   positive      0.759     0.800     0.779       444\n",
      "\n",
      "avg / total      0.769     0.768     0.768       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,                      # Free to write your own!\n",
    "    fit_maxent_with_crossvalidation,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func)  # Fixed.   uni_phi-0.772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 7–8: Bidirectional RNN [2 points]\n",
    "\n",
    "The auxiliary notebook `tensorflow_models.ipynb` [subclasses TfRNNClassifier with a bidirectional RNN](tensorflow_models.ipynb#A-bidirectional-RNN-Classifier). In this model, the RNN is run in both directions and the concatenation of the two final states is used as the basis for the classification decision. Evaluate this model against the SST dev set. You can set up the model however you wish for this.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "* Your call to `TfBidirectionalRNNClassifier` (so that we can see the hyperparmeters you chose).\n",
    "* Your average F1 score according to a `classification_report` on the `dev` set.\n",
    "\n",
    "As above, we will not evaluate you based on how good your F1 score is. You just need to submit one. __There is even value in seeing what really doesn't work__, so low scores have interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfBidirectionalRNNClassifier(TfRNNClassifier):\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self._define_embedding()\n",
    "\n",
    "        self.inputs = tf.placeholder(\n",
    "            tf.int32, [None, self.max_length])\n",
    "\n",
    "        self.ex_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        # Outputs as usual:\n",
    "        self.outputs = tf.placeholder(\n",
    "            tf.float32, shape=[None, self.output_dim])\n",
    "\n",
    "        # This converts the inputs to a list of lists of dense vector\n",
    "        # representations:\n",
    "        self.feats = tf.nn.embedding_lookup(\n",
    "            self.embedding, self.inputs)\n",
    "\n",
    "        # Same cell structure as the base class, but we have\n",
    "        # forward and backward versions:\n",
    "        self.cell_fw = tf.nn.rnn_cell.LSTMCell(\n",
    "            self.hidden_dim, activation=self.hidden_activation)\n",
    "        \n",
    "        self.cell_bw = tf.nn.rnn_cell.LSTMCell(\n",
    "            self.hidden_dim, activation=self.hidden_activation)\n",
    "\n",
    "        # Run the RNN:\n",
    "        outputs, finals = tf.nn.bidirectional_dynamic_rnn(\n",
    "            self.cell_fw,\n",
    "            self.cell_bw,\n",
    "            self.feats,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.ex_lengths)\n",
    "      \n",
    "        # finals is a pair of `LSTMStateTuple` objects, which are themselves\n",
    "        # pairs of Tensors (x, y), where y is the output state, according to\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "        # Thus, we want the second member of these pairs:\n",
    "        last_fw, last_bw = finals          \n",
    "        last_fw, last_bw = last_fw[1], last_bw[1]\n",
    "        \n",
    "        last = tf.concat((last_fw, last_bw), axis=1)\n",
    "        \n",
    "        self.feat_dim = self.hidden_dim * 2               \n",
    "\n",
    "        # Softmax classifier on the final hidden state:\n",
    "        self.W_hy = self.weight_init(\n",
    "            self.feat_dim, self.output_dim, 'W_hy')\n",
    "        self.b_y = self.bias_init(self.output_dim, 'b_y')\n",
    "        self.model = tf.matmul(last, self.W_hy) + self.b_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn_train, y_rnn_train = sst.build_binary_rnn_dataset(sst.train_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn_dev, y_rnn_dev = sst.build_binary_rnn_dataset(sst.dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train_vocab = sst.get_vocab(X_rnn_train, n_words=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_glove_vocab = sorted(set(glove_lookup) & set(sst_train_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = np.array([glove_lookup[w] for w in sst_glove_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add $UNK and its random representation:\n",
    "\n",
    "sst_glove_vocab.append(\"$UNK\")\n",
    "\n",
    "glove_embedding = np.vstack(\n",
    "    (glove_embedding, utils.randvec(glove_embedding.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bi_rnn = TfBidirectionalRNNClassifier(\n",
    "    sst_glove_vocab,\n",
    "    embedding=glove_embedding,\n",
    "    embed_dim=70,\n",
    "    hidden_dim=200,\n",
    "    max_length=52,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=500,\n",
    "    eta=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 500: loss: 3.5649400055408486"
     ]
    }
   ],
   "source": [
    "_ = tf_bi_rnn.fit(X_rnn_train, y_rnn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rnn_dev_predictions = tf_bi_rnn.predict(X_rnn_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.84      0.53      0.65       428\n",
      "   positive       0.67      0.90      0.77       444\n",
      "\n",
      "avg / total       0.75      0.72      0.71       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_rnn_dev, tf_rnn_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
